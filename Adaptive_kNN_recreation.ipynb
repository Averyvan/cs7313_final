{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7Z8mpFA6M7tB",
        "WcK9J7hxVb8N",
        "Y3n4VeOGaqHl",
        "7f8rByr9dW8k",
        "q5_xW91dz0hq",
        "vl3uZiUNDQ-y",
        "Ya_QDu0rFQAl"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive kNN\n",
        "\n",
        "## ACKER: Adaptive Classifier based on kNN Expected Accuracy\n",
        "\n",
        "```\n",
        "Algorithm 1 - ACKER\n",
        "===============================================\n",
        "Data: point p, set Range, function f, integer l\n",
        "Result: Class of point p\n",
        "===============================================\n",
        "optimal_k = 0\n",
        "max_accuracy = -1\n",
        "for k in Range do\n",
        "    candidate_accuracy = expected_accuracy(p, k, f, l)\n",
        "    if candidate_accuracy > max_accuracy then\n",
        "        optimal_k = k\n",
        "        max_accuracy = candidate_accuracy\n",
        "    end\n",
        "end\n",
        "predicted_class = kNN(p, optimal_k)\n",
        "return (predicted_class)\n",
        "```\n",
        "\n",
        "```\n",
        "Algorithm 2 - Expected Accuracy\n",
        "===============================================\n",
        "Data: point p, integer k, function f, integer l\n",
        "Result: Expected Accuracy for kNN(p, k)\n",
        "===============================================\n",
        "Find set C_(sim,l)(p,k) of l different points p' with minimal difference to reference function w.r.t. f:\n",
        "    sim(p, p') = |f(p,k) - f(p',k)|,\n",
        "    C_(sim,l) = argmax(summation(sim(p,p',k)))\n",
        "correctly_predicted = number of correct predictions of kNN(p',k), where p' element_of C_(sim,l)(p,k)\n",
        "expected_accuracy = (correctly_predicted / l)\n",
        "return expected_accuracy\n",
        "```"
      ],
      "metadata": {
        "id": "tD4cG31xISwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "Lt8aiexd6InJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"datasets.zip\"\n",
        "!pip install geojson\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "## Read and process crimessf\n",
        "df = pd.read_csv(\"crimessf.csv\")\n",
        "# Keep only the most-common top 16 categories\n",
        "top16 = df['Category'].value_counts().head(16).index\n",
        "df_top16 = df[df['Category'].isin(top16)].copy()\n",
        "sf_features = df_top16[[\"X\", \"Y\"]]\n",
        "sf_targets = df_top16[\"Category\"]\n",
        "# Encode the targets\n",
        "le = LabelEncoder()\n",
        "sf_targets = le.fit_transform(sf_targets)\n",
        "\n",
        "## Read and process twitter8 and twitter11\n",
        "import geojson\n",
        "with open(\"social-pulse-milano.geojson\") as f:\n",
        "    gj = geojson.load(f)\n",
        "\n",
        "g_df = pd.DataFrame(gj['features'])\n",
        "# Filter to only samples with non-empty entity lists\n",
        "g_df = g_df[g_df['entities'].apply(lambda x: len(x) > 0)].reset_index(drop=True)\n",
        "# Add X and Y columns to g_df based on geomPoint.geom Point values\n",
        "g_df['X'] = g_df['geomPoint.geom'].apply(lambda x: x[\"coordinates\"][0])\n",
        "g_df['Y'] = g_df['geomPoint.geom'].apply(lambda x: x[\"coordinates\"][1])\n",
        "# Explode targets\n",
        "g_df = g_df.explode('entities')\n",
        "\n",
        "# twitter8 had 6,230 items with 7 local geographical items + \"Italy\" as a general noise feature\n",
        "top_places8 = [\"http://dbpedia.org/resource/Italy\", # General noise feature, 1399\n",
        "              \"http://it.dbpedia.org/resource/Stadio_Giuseppe_Meazza\", # San Siro Stadium, 1615 mentions\n",
        "              \"http://it.dbpedia.org/resource/Arcidiocesi_di_Milano\", # Archdiocese of Milan, a region? 1447 mentions https://en.wikipedia.org/wiki/Archdiocese_of_Milan\n",
        "              \"http://it.dbpedia.org/resource/Duomo_di_Milano\", # Milan Cathedral, 889 mentions\n",
        "              \"http://it.dbpedia.org/resource/Aeroporto_di_Milano-Linate\", # Milan Linate city Airport, 704 mentions\n",
        "              \"http://it.dbpedia.org/resource/Stazione_di_Milano_Centrale\", # Milano Centrale Railway Station, 655 mentions\n",
        "              \"http://it.dbpedia.org/resource/Piazza_del_Duomo_%28Milano%29\", # Cathedral Square (Milan), 654 mentions\n",
        "              #\"http://it.dbpedia.org/resource/Enrico_Forlanini\", # Pretty sure this means Linate city airport but I think authors missed it, 643 mentions\n",
        "              \"http://it.dbpedia.org/resource/AF_-_L%27Artigiano_in_Fiera\", # \"Craftsman at the Fair\" trade fair, 390 mentions\n",
        "              #\"http://it.dbpedia.org/resource/Italia\",\n",
        "              ] # 7753 exploded samples, from 6248 tweets\n",
        "\n",
        "#Only get samples that have entities in top_places8\n",
        "top8_g_df = g_df[g_df['entities'].isin(top_places8)]\n",
        "# Extract features\n",
        "twitter8_features = top8_g_df[['X', 'Y']]\n",
        "twitter8_targets = top8_g_df['entities']\n",
        "# Encode targets\n",
        "le = LabelEncoder()\n",
        "twitter8_targets = le.fit_transform(twitter8_targets)\n",
        "\n",
        "# twitter11 had 21,658 items with 7 local geographical items + \"Italy\" as a general noise feature + 3 more unnamed general features\n",
        "top_places11 = [\"http://dbpedia.org/resource/Italy\", # General noise feature, 1399\n",
        "              \"http://it.dbpedia.org/resource/Stadio_Giuseppe_Meazza\", # San Siro Stadium, 1615 mentions\n",
        "              \"http://it.dbpedia.org/resource/Arcidiocesi_di_Milano\", # Archdiocese of Milan, a region? 1447 mentions https://en.wikipedia.org/wiki/Archdiocese_of_Milan\n",
        "              \"http://it.dbpedia.org/resource/Duomo_di_Milano\", # Milan Cathedral, 889 mentions\n",
        "              \"http://it.dbpedia.org/resource/Aeroporto_di_Milano-Linate\", # Milan Linate city Airport, 704 mentions\n",
        "              \"http://it.dbpedia.org/resource/Stazione_di_Milano_Centrale\", # Milano Centrale Railway Station, 655 mentions\n",
        "              \"http://it.dbpedia.org/resource/Piazza_del_Duomo_%28Milano%29\", # Cathedral Square (Milan), 654 mentions\n",
        "              #\"http://it.dbpedia.org/resource/Enrico_Forlanini\", # Pretty sure this means Linate city airport but I think authors missed it, 643 mentions\n",
        "              \"http://it.dbpedia.org/resource/AF_-_L%27Artigiano_in_Fiera\", # \"Craftsman at the Fair\" trade fair, 390 mentions\n",
        "              #\"http://it.dbpedia.org/resource/Italia\",\n",
        "              \"http://it.dbpedia.org/resource/Milano\",\n",
        "              \"http://it.dbpedia.org/resource/Associazione_Calcio_Milan\",\n",
        "              \"http://it.dbpedia.org/resource/Football_Club_Internazionale_Milano\",\n",
        "              ]\n",
        "\n",
        "#Only get samples that have entities in top_places11\n",
        "top11_g_df = g_df[g_df['entities'].isin(top_places11)]\n",
        "print(f\"Top places {top11_g_df.shape=}\")\n",
        "# Extract features\n",
        "top11_features = top11_g_df[['X', 'Y']]\n",
        "twitter11_targets = top11_g_df['entities']\n",
        "# Encode targets\n",
        "le = LabelEncoder()\n",
        "twitter11_targets = le.fit_transform(twitter11_targets)"
      ],
      "metadata": {
        "id": "iGgmvG7mWblc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recreating Figure 5 from the original paper\n",
        "\n",
        "Figure 5 shows the mean accuracy and its standard deviation of standard kNN algorithm dependent on the choice of k on our datasets"
      ],
      "metadata": {
        "id": "K1AGs3gwTv8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to recreate Figure 5 from the original paper (Fig. 2 in our report)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gather all accuracy values\n",
        "sf_accuracy_vs_k = dict()\n",
        "twitter8_accuracy_vs_k = dict()\n",
        "twitter11_accuracy_vs_k = dict()\n",
        "for k in range(1, 200):\n",
        "    # Make pipeline for this k\n",
        "    my_pipe = make_pipeline(\n",
        "        StandardScaler(),\n",
        "        KNeighborsClassifier(n_neighbors=k),\n",
        "    )\n",
        "    # Run on crimessf\n",
        "    sf_accuracy_vs_k[k] = np.mean(cross_val_score(my_pipe, sf_features, sf_targets, cv=10))\n",
        "    # Run on twitter8\n",
        "    twitter8_accuracy_vs_k[k] = np.mean(cross_val_score(my_pipe, twitter8_features, twitter8_targets, cv=10))\n",
        "    # Run on twitter11\n",
        "    twitter11_accuracy_vs_k[k] = np.mean(cross_val_score(my_pipe, top11_features, twitter11_targets, cv=10))\n",
        "\n",
        "# Print best accuracy k value and accuracy for each set\n",
        "print(f\"twitter8 best k: {max(twitter8_accuracy_vs_k, key=twitter8_accuracy_vs_k.get)}, acc={max(twitter8_accuracy_vs_k.values())}\")\n",
        "print(f\"twitter11 best k: {max(twitter11_accuracy_vs_k, key=twitter11_accuracy_vs_k.get)}, acc={max(twitter11_accuracy_vs_k.values())}\")\n",
        "print(f\"crimessf best k: {max(sf_accuracy_vs_k, key=sf_accuracy_vs_k.get)}, acc={max(sf_accuracy_vs_k.values())}\")\n",
        "\n",
        "# Create a figure with three subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 10.0/3.0), sharex=True, sharey=False)\n",
        "\n",
        "# Plot twitter8_accuracy_vs_k\n",
        "axes[0].plot(list(twitter8_accuracy_vs_k.keys()), list(twitter8_accuracy_vs_k.values()))\n",
        "axes[0].set_title(\"twitter8\", fontweight='bold')\n",
        "axes[0].set_xlabel(\"k\")\n",
        "axes[0].set_ylabel(\"Accuracy\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot twitter11_accuracy_vs_k\n",
        "axes[1].plot(list(twitter11_accuracy_vs_k.keys()), list(twitter11_accuracy_vs_k.values()))\n",
        "axes[1].set_title(\"twitter11\", fontweight='bold')\n",
        "axes[1].set_xlabel(\"k\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].grid(True)\n",
        "\n",
        "# Plot SF_accuracy_vs_k\n",
        "axes[2].plot(list(sf_accuracy_vs_k.keys()), list(sf_accuracy_vs_k.values()))\n",
        "axes[2].set_title(\"crimessf\", fontweight='bold')\n",
        "axes[2].set_xlabel(\"k\")\n",
        "axes[2].set_ylabel(\"Accuracy\")\n",
        "axes[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig.savefig(\"Figure5_all_datasets_accuracy_vs_k.pdf\")"
      ],
      "metadata": {
        "id": "aQS6--QRu3Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MaxHeap (prerequisite)"
      ],
      "metadata": {
        "id": "0Hpmx70NOZF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import math\n",
        "from typing import Callable\n",
        "import numpy as np\n",
        "\n",
        "class MaxHeap:\n",
        "    \"\"\"\n",
        "    Max Heaps allow tracking of the n smallest values in a list because the \"maximum of the smallest numbers\" is always at the root.\n",
        "    \"\"\"\n",
        "    # Initialize the max heap\n",
        "    def __init__(self, data=None):\n",
        "        if data is None:\n",
        "            self.data = []\n",
        "        else:\n",
        "            self.data = [-i for i in data]\n",
        "            heapq.heapify(self.data)\n",
        "\n",
        "    # Push item onto max heap, maintaining the heap invariant\n",
        "    def push(self, item):\n",
        "        heapq.heappush(self.data, -item)\n",
        "\n",
        "    # Pop the largest item off the max heap, maintaining the heap invariant\n",
        "    def pop(self):\n",
        "        return -heapq.heappop(self.data)\n",
        "\n",
        "    # Pop and return the current largest value, and add the new item\n",
        "    def replace(self, item):\n",
        "        return heapq.heapreplace(self.data, -item)\n",
        "\n",
        "    # Return the current largest value in the max heap\n",
        "    def top(self):\n",
        "        return -self.data[0]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __repr__(self):\n",
        "        listToStr = \" \".join([str(elem) for elem in self.data])\n",
        "        return listToStr + \"\\n\"\n",
        "\n",
        "\n",
        "class Pair:\n",
        "    \"\"\"\n",
        "    Override the less-than operator __lt__ to make Pair class work with max heap\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, distance, index):\n",
        "        self.distance = distance\n",
        "        self.index = index\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.distance < other.distance\n",
        "\n",
        "    def __gt__(self, other):\n",
        "        return self.distance > other.distance\n",
        "\n",
        "    def __neg__(self):\n",
        "        return Pair(-self.distance, self.index)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"({self.distance}, {self.index})\""
      ],
      "metadata": {
        "id": "GuQ_xkfdCtcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B+ Tree Implementation (prerequisite)"
      ],
      "metadata": {
        "id": "Zz0_snQqOxdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source: https://gist.github.com/savarin/69acd246302567395f65ad6b97ee503d\n",
        "# Adapted to have inter-leaf pointers for fast retrieval of most-similar items\n",
        "\"\"\"Simple implementation of a B+ tree, a self-balancing tree data structure that (1) maintains sort\n",
        "data order and (2) allows insertions and access in logarithmic time.\n",
        "\"\"\"\n",
        "class Node(object):\n",
        "    \"\"\"Base node object.\n",
        "    Each node stores keys and values. Keys are not unique to each value, and as such values are\n",
        "    stored as a list under each key.\n",
        "    Attributes:\n",
        "        order (int): The maximum number of keys each node can hold.\n",
        "    \"\"\"\n",
        "    def __init__(self, order):\n",
        "        \"\"\"Child nodes can be converted into parent nodes by setting self.leaf = False. Parent nodes\n",
        "        simply act as a medium to traverse the tree.\"\"\"\n",
        "        self.order = order\n",
        "        self.keys = []\n",
        "        self.values = []\n",
        "        self.leaf = True\n",
        "        self.next = None\n",
        "        self.prev = None\n",
        "\n",
        "    def add(self, key, value):\n",
        "        \"\"\"Adds a key-value pair to the node.\"\"\"\n",
        "        # If the node is empty, simply insert the key-value pair.\n",
        "        if not self.keys:\n",
        "            self.keys.append(key)\n",
        "            self.values.append([value])\n",
        "            return None\n",
        "\n",
        "        for i, item in enumerate(self.keys):\n",
        "            # If new key matches existing key, add to list of values.\n",
        "            if key == item:\n",
        "                self.values[i].append(value)\n",
        "                break\n",
        "\n",
        "            # If new key is smaller than existing key, insert new key to the left of existing key.\n",
        "            elif key < item:\n",
        "                self.keys = self.keys[:i] + [key] + self.keys[i:]\n",
        "                self.values = self.values[:i] + [[value]] + self.values[i:]\n",
        "                break\n",
        "\n",
        "            # If new key is larger than all existing keys, insert new key to the right of all\n",
        "            # existing keys.\n",
        "            elif i + 1 == len(self.keys):\n",
        "                self.keys.append(key)\n",
        "                self.values.append([value])\n",
        "\n",
        "    def split(self):\n",
        "        \"\"\"Splits the node into two and stores them as child nodes.\"\"\"\n",
        "        left = Node(self.order)\n",
        "        right = Node(self.order)\n",
        "        mid = self.order // 2\n",
        "\n",
        "        # Fix incoming pointers\n",
        "        if self.prev:\n",
        "            self.prev.next = left\n",
        "        if self.next:\n",
        "            self.next.prev = right\n",
        "\n",
        "        left.keys = self.keys[:mid]\n",
        "        left.values = self.values[:mid]\n",
        "        left.next = right\n",
        "        left.prev = self.prev\n",
        "\n",
        "        right.keys = self.keys[mid:]\n",
        "        right.values = self.values[mid:]\n",
        "        right.next = self.next\n",
        "        right.prev = left\n",
        "\n",
        "        # When the node is split, set the parent key to the left-most key of the right child node.\n",
        "        self.keys = [right.keys[0]]\n",
        "        self.values = [left, right]\n",
        "        self.leaf = False\n",
        "\n",
        "    def is_full(self):\n",
        "        \"\"\"Returns True if the node is full.\"\"\"\n",
        "        return len(self.keys) == self.order\n",
        "\n",
        "    def show(self, counter=0):\n",
        "        \"\"\"Prints the keys at each level.\"\"\"\n",
        "        print(counter, str(self.keys))\n",
        "\n",
        "        # Recursively print the key of child nodes (if these exist).\n",
        "        if not self.leaf:\n",
        "            for item in self.values:\n",
        "                item.show(counter + 1)\n",
        "\n",
        "class BPlusTree(object):\n",
        "    \"\"\"B+ tree object, consisting of nodes.\n",
        "\n",
        "    Nodes will automatically be split into two once it is full. When a split occurs, a key will\n",
        "    'float' upwards and be inserted into the parent node to act as a pivot.\n",
        "\n",
        "    Attributes:\n",
        "        order (int): The maximum number of keys each node can hold.\n",
        "    \"\"\"\n",
        "    def __init__(self, order=8):\n",
        "        self.root = Node(order)\n",
        "\n",
        "    def _find(self, node, key):\n",
        "        \"\"\" For a given node and key, returns the index where the key should be inserted and the\n",
        "        list of values at that index.\"\"\"\n",
        "        for i, item in enumerate(node.keys):\n",
        "            if key < item:\n",
        "                return node.values[i], i    # left child, index   (if not leaf, values holds [left, right] nodes)\n",
        "\n",
        "        return node.values[i + 1], i + 1    # right child, index\n",
        "\n",
        "    def _merge(self, parent, child, index):\n",
        "        \"\"\"For a parent and child node, extract a pivot from the child to be inserted into the keys\n",
        "        of the parent. Insert the values from the child into the values of the parent.\n",
        "        \"\"\"\n",
        "        parent.values.pop(index)\n",
        "        pivot = child.keys[0]\n",
        "\n",
        "        for i, item in enumerate(parent.keys):\n",
        "            if pivot < item:\n",
        "                parent.keys = parent.keys[:i] + [pivot] + parent.keys[i:]\n",
        "                parent.values = parent.values[:i] + child.values + parent.values[i:]\n",
        "                break\n",
        "\n",
        "            elif i + 1 == len(parent.keys):\n",
        "                parent.keys += [pivot]\n",
        "                parent.values += child.values\n",
        "                break\n",
        "\n",
        "    def insert(self, key, value):\n",
        "        \"\"\"Inserts a key-value pair after traversing to a leaf node. If the leaf node is full, split\n",
        "        the leaf node into two.\n",
        "        \"\"\"\n",
        "        parent = None\n",
        "        child = self.root\n",
        "\n",
        "        # Traverse tree until leaf node is reached.\n",
        "        while not child.leaf:\n",
        "            parent = child\n",
        "            child, index = self._find(child, key)\n",
        "\n",
        "        child.add(key, value)\n",
        "\n",
        "        # If the leaf node is full, split the leaf node into two.\n",
        "        if child.is_full():\n",
        "            child.split()\n",
        "\n",
        "            # Once a leaf node is split, it consists of a internal node and two leaf nodes. These\n",
        "            # need to be re-inserted back into the tree.\n",
        "            if parent and not parent.is_full():\n",
        "                self._merge(parent, child, index)\n",
        "\n",
        "    def retrieve(self, key):\n",
        "        \"\"\"Returns a value for a given key, and None if the key does not exist.\"\"\"\n",
        "        child = self.root\n",
        "\n",
        "        while not child.leaf:\n",
        "            child, index = self._find(child, key)\n",
        "\n",
        "        for i, item in enumerate(child.keys):\n",
        "            if key == item:\n",
        "                return child.values[i]\n",
        "\n",
        "        return None\n",
        "\n",
        "    # AV: Added this function and the \"next\" and \"prev\" pointers\n",
        "    def find_nearest(self, key, k):\n",
        "        \"\"\"Returns up to k*2 values nearest to a given key.\"\"\"\n",
        "        child = self.root\n",
        "\n",
        "        # Find leaf node where key should go\n",
        "        while not child.leaf:\n",
        "            child, index = self._find(child, key)\n",
        "\n",
        "        max_item = child.keys[-1]\n",
        "        for i, item in enumerate(child.keys):\n",
        "            if key < item or item == max_item:\n",
        "                center = child\n",
        "                assert(center.leaf == True)\n",
        "\n",
        "                lesser_keys = []\n",
        "                lesser_values = []\n",
        "                added_lesser = 0\n",
        "                for j in range(i-1, -1, -1): # iterate backwards through node\n",
        "                    num_values = len(child.values[j]) # number of samples with the same value\n",
        "                    lesser_values.extend(child.values[j]) # add all indices with this value\n",
        "                    lesser_keys.extend([child.keys[j]] * num_values) # insert the key for every index added\n",
        "                    added_lesser += num_values\n",
        "                    if added_lesser >= k:\n",
        "                        break\n",
        "                child = child.prev\n",
        "                while added_lesser < k and child is not None:\n",
        "                    while child.leaf == False: # Find closest leaf\n",
        "                        child = child.values[1] # Go to right child\n",
        "                    for j in range(len(child.values)-1, -1, -1): # iterate backwards through node\n",
        "                        num_values = len(child.values[j]) # number of samples with the same value\n",
        "                        lesser_values.extend(child.values[j]) # add all indices with this value\n",
        "                        lesser_keys.extend([child.keys[j]] * num_values) # insert the key for every index added\n",
        "                        added_lesser += num_values\n",
        "                        if added_lesser >= k:\n",
        "                            break\n",
        "                    child = child.prev\n",
        "\n",
        "                child = center # reset back to pivot\n",
        "\n",
        "                greater_keys = []\n",
        "                greater_values = []\n",
        "                added_greater = 0\n",
        "                for j in range(i+1, len(child.keys)):\n",
        "                    num_values = len(child.values[j])\n",
        "                    greater_values.extend(child.values[j])\n",
        "                    greater_keys.extend([child.keys[j]] * num_values)\n",
        "                    added_greater += num_values\n",
        "                    if added_greater >= k:\n",
        "                        break\n",
        "                child = child.next\n",
        "                while added_greater < k and child is not None:\n",
        "                    while child.leaf == False: # Find closest leaf\n",
        "                        child = child.values[0] # Go to left child\n",
        "                    for j in range(len(child.values)):\n",
        "                        num_values = len(child.values[j])\n",
        "                        greater_values.extend(child.values[j])\n",
        "                        greater_keys.extend([child.keys[j]] * num_values)\n",
        "                        added_greater += num_values\n",
        "                        if added_greater >= k:\n",
        "                            break\n",
        "                    child = child.next\n",
        "\n",
        "                all_keys = lesser_keys + greater_keys       # Distances\n",
        "                all_values = lesser_values + greater_values # Indices\n",
        "                all_pairs = []\n",
        "                # Build Pairs of (key, value)\n",
        "                for i in range(len(all_keys)):\n",
        "                    all_pairs.append(Pair(all_keys[i], all_values[i]))\n",
        "                return all_pairs\n",
        "\n",
        "        print(\"ERROR in find_nearest? returning no similar values\")\n",
        "        return []\n",
        "\n",
        "    def show(self):\n",
        "        \"\"\"Prints the keys at each level.\"\"\"\n",
        "        self.root.show()\n"
      ],
      "metadata": {
        "id": "ADOBzK0HScx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Adaptive kNN Implementation"
      ],
      "metadata": {
        "id": "1uIDhf0k6jJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "class AdaptiveFastKNNClassifier(object):\n",
        "    \"\"\"\n",
        "    Adaptive Fast KNN Classifier implemented from the paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_k=15):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            values_of_k_as_set (set): \"Range\" ⊆ N defines the set of candidates k.\n",
        "            similarity_function (Callable) : \"SimilarityFunction defines which similarity function should be used for estimation of the expected accuracy.\"\n",
        "            number_of_similar_points_to_find (int): \"l is the parameter for the expected accuracy estimation\"\n",
        "            dist_fn (Callable): distance function to use for either/both of ACKER and normal kNN?\n",
        "                NOTE: This is just Euclidean distance\n",
        "        \"\"\"\n",
        "\n",
        "        self.max_k = max_k\n",
        "        self.values_of_k_as_set = {i for i in range(2, max_k)}\n",
        "        self.k_to_index_map = {k: i for i, k in enumerate(self.values_of_k_as_set)}\n",
        "        self.num_k = len(self.values_of_k_as_set)\n",
        "\n",
        "        # self.similarity_scorer = self.lat_lon\n",
        "        self.similarity_scorer = self.ave_dist\n",
        "        # self.similarity_scorer = self.max_dist\n",
        "        # self.similarity_scorer = self.max_ave_comb\n",
        "\n",
        "        self.number_of_similar_points_to_find = 1\n",
        "\n",
        "\n",
        "    def dist_fn(self, a, b):\n",
        "        return np.linalg.norm(np.array(a) - np.array(b))\n",
        "\n",
        "    def _find_nearest(self, x, k):\n",
        "        \"\"\"\n",
        "        Find the k nearest neighbors of point x\n",
        "        Returns a list of (distance, index) pairs\n",
        "        \"\"\"\n",
        "        maxheap = MaxHeap()\n",
        "        for i in range(k):\n",
        "          maxheap.push(Pair(self.dist_fn(x, self.dataset_[i]), i))\n",
        "        for j in range(k, self.dataset_.shape[0]):\n",
        "            new_neighbor = Pair(self.dist_fn(x, self.dataset_[j]), j)\n",
        "            if new_neighbor < maxheap.top():\n",
        "              maxheap.replace(new_neighbor)\n",
        "\n",
        "        dist_idx_pairs = []\n",
        "        while maxheap.size():\n",
        "          neighbor = maxheap.pop()\n",
        "          dist_idx_pairs.append((neighbor.distance, neighbor.index))\n",
        "        return dist_idx_pairs\n",
        "\n",
        "    def lat_lon(self, p):\n",
        "        return [(p[0], p[1]) for _ in range(self.num_k)]\n",
        "\n",
        "    def ave_dist(self, p):\n",
        "        max_k_nearest = self._find_nearest(p, self.max_k)\n",
        "        scores = []\n",
        "        for k in self.values_of_k_as_set:\n",
        "            distances = [entry[0] for entry in max_k_nearest[:k]]\n",
        "            scores.append(np.mean(distances))\n",
        "        return scores\n",
        "\n",
        "    def max_dist(self, p):\n",
        "        max_k_nearest = self._find_nearest(p, self.max_k)\n",
        "        scores = []\n",
        "        for k in self.values_of_k_as_set:\n",
        "            scores.append(max_k_nearest[k - 1][0])\n",
        "        return scores\n",
        "\n",
        "    def max_ave_comb(self, p):\n",
        "        max_k_nearest = self._find_nearest(p, self.max_k)\n",
        "        scores = []\n",
        "        for k in self.values_of_k_as_set:\n",
        "            distances = [entry[0] for entry in max_k_nearest[:k]]\n",
        "            mean = np.mean(distances)\n",
        "            scores.append((max_k_nearest[k - 1][0], mean))\n",
        "        return scores\n",
        "\n",
        "    def lat_lon_max_ave_comb(self, p):\n",
        "        max_k_nearest = self._find_nearest(p, self.max_k)\n",
        "        scores = []\n",
        "        for k in self.values_of_k_as_set:\n",
        "            distances = [entry[0] for entry in max_k_nearest[:k]]\n",
        "            mean = np.mean(distances)\n",
        "            scores.append((p[0], p[1], max_k_nearest[k - 1][0], mean))\n",
        "        return scores\n",
        "\n",
        "    def expected_accuracy(self, p_scores, k):\n",
        "        \"\"\"\n",
        "        Expected Accuracy for kNN(p, k)\n",
        "        \"\"\"\n",
        "\n",
        "        p_score = p_scores[self.k_to_index_map[k]]\n",
        "\n",
        "        if self.similarity_scorer == self.lat_lon: # Can't use B+ trees, compute similarity across all existing points\n",
        "            all_similarities = np.zeros(self.dataset_.shape[0])\n",
        "            for i in range(self.dataset_.shape[0]):\n",
        "                i_score = self.scores[i][self.k_to_index_map[k]]\n",
        "                all_similarities[i] = -self.dist_fn(p_score, i_score)\n",
        "\n",
        "            most_similar_l_points = np.argpartition(all_similarities, -self.number_of_similar_points_to_find)[-self.number_of_similar_points_to_find:]\n",
        "\n",
        "            # Compute the expected accuracy\n",
        "            self.f_kNN.n_neighbors = k\n",
        "            most_similar_X = self.dataset_[most_similar_l_points]\n",
        "            most_similar_y = self.labels_[most_similar_l_points]\n",
        "            y_pred = self.f_kNN.predict(most_similar_X)\n",
        "            correctly_predicted = (y_pred == most_similar_y).sum()\n",
        "\n",
        "            return correctly_predicted / self.number_of_similar_points_to_find\n",
        "\n",
        "        else: # Used B+ trees, find most-similar points using them\n",
        "\n",
        "            # Returns Pair objects (i_score, i) of the l*2 most similar points from the given k's B+ tree\n",
        "            most_similar = self.scores[self.k_to_index_map[k]].find_nearest(p_score, self.number_of_similar_points_to_find)\n",
        "            #print(f\"Looking for {self.number_of_similar_points_to_find}*2 most similar points, found {len(all_similarities)}:\\n{all_similarities=}\")\n",
        "            all_similarities = np.array([Pair(-self.dist_fn(p_score, pair.distance), pair.index) for pair in most_similar])\n",
        "\n",
        "            # Optimization: argpartition can find top-l similarities in linear time at worst\n",
        "            most_similar_l_points = np.argpartition(all_similarities, -self.number_of_similar_points_to_find)[-self.number_of_similar_points_to_find:]\n",
        "            most_similar_l_points = all_similarities[most_similar_l_points]\n",
        "\n",
        "            # Compute the expected accuracy\n",
        "            self.f_kNN.n_neighbors = k\n",
        "            most_similar_X = [self.dataset_[pair.index] for pair in most_similar_l_points]\n",
        "            most_similar_y = [self.labels_[pair.index] for pair in most_similar_l_points]\n",
        "            y_pred = self.f_kNN.predict(most_similar_X)\n",
        "            correctly_predicted = (y_pred == most_similar_y).sum()\n",
        "\n",
        "            return correctly_predicted / self.number_of_similar_points_to_find\n",
        "\n",
        "\n",
        "    def ACKER(self, p):\n",
        "        \"\"\"\n",
        "        Adaptive Classifier based on kNN Expected Accuracy\n",
        "\n",
        "        \"The main idea of the ACKER algorithm is to find optimal k for\n",
        "        each point (instance) and to use this k with standard kNN, (see\n",
        "        Section 4.1)\"\n",
        "\n",
        "        \"Given a point p, the algorithm estimates the expected\n",
        "        accuracy for different possible values of k, and chooses a k that\n",
        "        provides a maximum value of the expected accuracy, (see Section 4.2).\"\n",
        "\n",
        "        \"...we need to find k so that exp_acc(p,k) = max{exp_acc(p,k)|k ∈ Range},\n",
        "        where Range is defined as the range of possible values for k. The\n",
        "        standard kNN classifier with optimal k is applied afterwards.\"\n",
        "\n",
        "        Args:\n",
        "            p: Point to classify\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Find optimal_k for point\n",
        "        optimal_k = 0\n",
        "        max_accuracy = -1\n",
        "        p_scores = self.similarity_scorer(p)\n",
        "        for k in self.values_of_k_as_set:\n",
        "            candidate_accuracy = self.expected_accuracy(p_scores, k)\n",
        "            if candidate_accuracy > max_accuracy:\n",
        "                optimal_k = k\n",
        "                max_accuracy = candidate_accuracy\n",
        "\n",
        "        # Step 2: Use this optimal_k with standard kNN\n",
        "        self.f_kNN.n_neighbors = optimal_k\n",
        "        predicted_class = self.f_kNN.predict(np.expand_dims(p, axis=0))\n",
        "        predicted_class = np.squeeze(predicted_class, axis=0)\n",
        "        return predicted_class\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.dataset_ = X.copy()\n",
        "        self.labels_ = y.copy()\n",
        "\n",
        "        # For each training sample, precompute scores for each k\n",
        "        if self.similarity_scorer == self.lat_lon: # Can't use B+ trees for lat_lon\n",
        "            self.scores = [[] for _ in range(self.dataset_.shape[0])]\n",
        "            for i in range(self.dataset_.shape[0]):\n",
        "              self.scores[i] = self.similarity_scorer(self.dataset_[i])\n",
        "        else:\n",
        "            self.scores = [BPlusTree(order=32) for _ in range(self.num_k)]\n",
        "            for i in range(self.dataset_.shape[0]):\n",
        "              scores = self.similarity_scorer(self.dataset_[i])\n",
        "              for j in range(self.num_k):\n",
        "                  self.scores[j].insert(scores[j], i)\n",
        "\n",
        "        self.f_kNN = KNeighborsClassifier(n_neighbors=1) # Fit once and adjust n_neighbors as needed\n",
        "        self.f_kNN.fit(self.dataset_, self.labels_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.zeros(X.shape[0], dtype=int)\n",
        "        for i in range(X.shape[0]):\n",
        "            predicted_label = self.ACKER(X[i])\n",
        "            predictions[i] = predicted_label\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        num_correct_predictions = (y_pred == y).sum()\n",
        "        score = num_correct_predictions / y.shape[0]\n",
        "\n",
        "        return score"
      ],
      "metadata": {
        "id": "5I-BwvEeCqNv"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# twitter8 experiments"
      ],
      "metadata": {
        "id": "yOvC4nGGS5db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full twitter8 lat_lon CV=10\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.lat_lon\n",
        "my_knn.number_of_similar_points_to_find = 1\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Cross-validate\n",
        "scores = cross_val_score(my_pipe, twitter8_features, twitter8_targets, cv=10)\n",
        "print(f\"{scores=}\")\n",
        "acc = scores.mean()\n",
        "print(f\"Cross-validated mean accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btbuMynWW1sN",
        "outputId": "ee9b6bad-ddd7-4cb2-8380-cffdc082964c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full twitter8 lat_lon\n",
            "scores=array([0.70360825, 0.70231959, 0.6314433 , 0.67612903, 0.58580645,\n",
            "       0.76387097, 0.68129032, 0.57419355, 0.80645161, 0.72129032])\n",
            "acc=np.float64(0.6846403392085135)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full twitter8 ave_dist CV=10\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.ave_dist\n",
        "my_knn.number_of_similar_points_to_find = 175\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Cross-validate\n",
        "scores = cross_val_score(my_pipe, twitter8_features, twitter8_targets, cv=10)\n",
        "print(f\"{scores=}\")\n",
        "acc = scores.mean()\n",
        "print(f\"Cross-validated mean accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwPw_bAbW-rO",
        "outputId": "a68643c3-59a2-4a90-961c-550b6d1ce129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full twitter8 ave_dist CV=10\n",
            "scores=array([0.70876289, 0.74613402, 0.63530928, 0.74193548, 0.70322581,\n",
            "       0.78709677, 0.72129032, 0.73935484, 0.80774194, 0.72387097])\n",
            "acc=np.float64(0.7314722314599268)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full twitter8 max_dist CV=10\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.max_dist\n",
        "my_knn.number_of_similar_points_to_find = 169\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Cross-validate\n",
        "scores = cross_val_score(my_pipe, twitter8_features, twitter8_targets, cv=10)\n",
        "print(f\"{scores=}\")\n",
        "acc = scores.mean()\n",
        "print(f\"Cross-validated mean accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlX8O4svZEZz",
        "outputId": "2d514e96-12e8-4260-e15d-2cc92a13f056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full twitter8 max_dist CV=10\n",
            "scores=array([0.70103093, 0.7242268 , 0.62113402, 0.73806452, 0.69935484,\n",
            "       0.73419355, 0.71741935, 0.73548387, 0.80645161, 0.72129032])\n",
            "acc=np.float64(0.719864981709345)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full twitter8 max_ave_comb CV=10\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.max_ave_comb\n",
        "my_knn.number_of_similar_points_to_find = 113\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Cross-validate\n",
        "scores = cross_val_score(my_pipe, twitter8_features, twitter8_targets, cv=10)\n",
        "print(f\"{scores=}\")\n",
        "acc = scores.mean()\n",
        "print(f\"Cross-validated mean accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNXdlG9rZEN1",
        "outputId": "1d6360ea-980f-496a-fb1c-cb40495bb2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full twitter8 max_ave_comb CV=10\n",
            "scores=array([0.70747423, 0.7306701 , 0.63659794, 0.74193548, 0.70193548,\n",
            "       0.77806452, 0.72129032, 0.74451613, 0.81290323, 0.72774194])\n",
            "acc=np.float64(0.730312936481543)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Full twitter8 NEW lat_lon_max_ave_comb CV=10\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.lat_lon_max_ave_comb\n",
        "my_knn.number_of_similar_points_to_find = 113\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Cross-validate\n",
        "scores = cross_val_score(my_pipe, twitter8_features, twitter8_targets, cv=10)\n",
        "print(f\"{scores=}\")\n",
        "acc = scores.mean()\n",
        "print(f\"{acc=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzcNBl7H0Ewx",
        "outputId": "d36301d1-acf5-4a6f-c48e-5cd0a0fc5feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full twitter8 NEW lat_lon_max_ave_comb, CV=10 l=113\n",
            "scores=array([0.70360825, 0.74097938, 0.62886598, 0.73806452, 0.69935484,\n",
            "       0.78193548, 0.72774194, 0.73806452, 0.81419355, 0.73419355])\n",
            "acc=np.float64(0.7307001995344198)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# twitter11 experiments"
      ],
      "metadata": {
        "id": "EstqZuyfT2Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## First, shrink to subset of twitter11\n",
        "# Grab 50% of the twitter11 dataset, stratified\n",
        "less_features, _, less_targets, _ = train_test_split(\n",
        "    top11_features,\n",
        "    twitter11_targets,\n",
        "    train_size=0.50, # percent to keep\n",
        "    shuffle=True,\n",
        "    stratify=twitter11_targets)\n",
        "print(f\"{less_features.shape=}\")\n",
        "print(f\"{less_targets.shape=}\")\n",
        "\n",
        "# Split the data subset 90:10\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    less_features,\n",
        "    less_targets,\n",
        "    test_size=0.1,\n",
        "    shuffle=True,\n",
        "    stratify=less_targets)"
      ],
      "metadata": {
        "id": "oTGyf2GgT-bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Half twitter11 lat_lon 10% holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.lat_lon\n",
        "my_knn.number_of_similar_points_to_find = 1\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "id": "AG5DcGotXMwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e449ab-c84d-47b5-ef48-b4a169e1ba05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Half twitter11 lat_lon\n",
            "X_test.shape=(1392, 2)\n",
            "test_acc=0.5854885057471264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Half twitter11 ave_dist 10% holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.ave_dist\n",
        "my_knn.number_of_similar_points_to_find = 268\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlCtYj3Yp8Gh",
        "outputId": "7f0bf08a-01db-48c8-f59c-99cbe3c1dae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Half twitter11 ave_dist\n",
            "X_test.shape=(1392, 2)\n",
            "test_acc=0.6242816091954023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Half twitter11 max_dist 10% holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.max_dist\n",
        "my_knn.number_of_similar_points_to_find = 251\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCwT0_JRwY1D",
        "outputId": "a7c4c83a-e8d0-410b-ca70-57cecb67f022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Half twitter11 max_dist\n",
            "X_test.shape=(1392, 2)\n",
            "test_acc=0.6228448275862069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Half twitter11 max_ave_comb 10% holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.max_ave_comb\n",
        "my_knn.number_of_similar_points_to_find = 191\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh69DR7twfA4",
        "outputId": "93aa25c4-8435-4484-98de-d3fd907a377c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Half twitter11 max_ave_comb\n",
            "X_test.shape=(1392, 2)\n",
            "test_acc=0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Half twitter11 NEW lat_lon_max_ave_comb 10% holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.lat_lon_max_ave_comb\n",
        "my_knn.number_of_similar_points_to_find = 191\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0n9LXxm-9yZ",
        "outputId": "01483f50-8b65-4a29-b270-2af210d3557d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Half twitter11 NEW lat_lon_max_ave_comb\n",
            "X_test.shape=(1392, 2)\n",
            "test_acc=0.6142241379310345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# crimessf experiments"
      ],
      "metadata": {
        "id": "KIxGb2wDU7xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## First, shrink to subset of crimessf\n",
        "# Grab 10% of the crimessf dataset, stratified\n",
        "less_features, _, less_targets, _ = train_test_split(\n",
        "    sf_features,\n",
        "    sf_targets,\n",
        "    train_size=0.10, # Percent to KEEP\n",
        "    shuffle=True,\n",
        "    stratify=sf_targets)\n",
        "print(f\"{less_features.shape=}\")\n",
        "print(f\"{less_targets.shape=}\")\n",
        "\n",
        "# Split the data subset 90:10\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    less_features,\n",
        "    less_targets,\n",
        "    test_size=0.1,\n",
        "    shuffle=True,\n",
        "    stratify=less_targets)"
      ],
      "metadata": {
        "id": "yPGzsYB1VHZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"10% SF lat_lon single-holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.lat_lon\n",
        "my_knn.number_of_similar_points_to_find = 166\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "id": "-8AlassQDQpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"10% SF ave_dist single-holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.ave_dist\n",
        "my_knn.number_of_similar_points_to_find = 742\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "id": "Dc8Dtxh8GGHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"10% SF max_dist single-holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.max_dist\n",
        "my_knn.number_of_similar_points_to_find = 734\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "id": "ljyNYlvtHEE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"10% SF max_ave_comb single-holdout\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.max_ave_comb\n",
        "my_knn.number_of_similar_points_to_find = 290\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRmqgi9FHHs0",
        "outputId": "63da8767-6e78-40e4-9ea2-874cfbfcade5"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5% SF max_ave_comb single-holdout\n",
            "X_test.shape=(702, 2)\n",
            "test_acc=0.24216524216524216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"10% SF lat_lon_max_ave_comb single-holdout l=290\")\n",
        "my_knn = AdaptiveFastKNNClassifier()\n",
        "my_knn.similarity_scorer = my_knn.lat_lon_max_ave_comb\n",
        "my_knn.number_of_similar_points_to_find = 290\n",
        "my_pipe = make_pipeline(StandardScaler(), my_knn)\n",
        "# Single 10% holdout\n",
        "my_pipe.fit(X_train, y_train)\n",
        "print(f\"{X_test.shape=}\")\n",
        "y_pred = my_pipe.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"{test_acc=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYUCe18aEeXC",
        "outputId": "ec2c59e1-6ed7-4e2e-9d33-f1ddf25b71fd"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10% SF lat_lon_max_ave_comb single-holdout l=290\n",
            "X_test.shape=(1403, 2)\n",
            "test_acc=0.2580185317177477\n"
          ]
        }
      ]
    }
  ]
}